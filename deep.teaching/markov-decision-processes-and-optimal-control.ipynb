{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes and Optimal Control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements)\n",
    "  * [Modules](#Python-Modules)\n",
    "* [Markov Decision Processes](#Markov-Decision-Processes)\n",
    "* [Summary and Outlook](#Summary-and-Outlook)\n",
    "* [Literature](#Literature)\n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Goal of this notebook is to introduce _Markov Decision Processes_ (MDPs) and how to solve them by _dynamic programming_. \n",
    "MDPs model the interaction between an agent and the environment.  \n",
    "\n",
    "In the full reinforcement learning problem the environment-agent model is unknown. However, an understanding of MDPs is fundamental for reinforcement learning.  \n",
    "\n",
    "<!-- Reinforcement Learning [[SUT98]](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) and present an overview of the principles and background.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Modules\n",
    "\n",
    "With the deep.TEACHING convention, all python modules needed to run the notebook are loaded centrally at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library\n",
    "import os\n",
    "\n",
    "# External Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import mpl_toolkits.mplot3d\n",
    "\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "Markov decision processes (MDP) are a model of the agent-enviroment interaction. \n",
    "\n",
    "In this notebook key elements of MDPs are introduced. \n",
    "\n",
    "### Definition\n",
    "A countable MDP is defined as a triple $\\mathcal M = (\\mathcal{S, A, P_{0}}) $ with\n",
    "\n",
    "* A countable not-empty set of states $s \\in \\mathcal S$.\n",
    "* Countable not-empty set of actions $a \\in \\mathcal A$.\n",
    "* Transition probability kernel $\\mathcal P_0$: $\\mathcal P_0$ describes the (stochastic) reaction of the enviroment to the agents action. For each state-action pair $(S=s, A=a) \\in \\mathcal S \\times \\mathcal A$ there is a probability distribution over $\\mathcal S \\times \\mathbb R$: $P_0(\\cdot \\mid s, a)$ with the following semantics: \n",
    "For $U \\in \\mathcal S \\times \\mathbb R$ is $P_0(U \\mid s, a)$ the probability that the next states and the reward $R$ belongs to the set $U$ given that the current state is $s$ and the action taken is $a$.  \n",
    "\n",
    "MDPs are models of the interaction between the enviroment and the agent."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment\n",
    "\n",
    "An agent takes an action in the given environment with the goal to maximize the long-term received reward (cumulative reward). This principle can be seen in figure 1 as an infinite loop.\n",
    "![image.png](attachment:image.png)\n",
    "<center>Figure 1 - the agent–environment interaction in a Markov decision process <a href=\"https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\">[SUT98]</a></center>\n",
    "\n",
    "At timestep $t$ the agent receives a state $S_{t}=s$ (for example a camera image) and performs an action $A_{t}=a$. In the next timestep $t+1$, a reward $R_{t+1}=r$ and the resulting new state $S_{t+1}=s'$ from the taken action $A_{t}=a$ of timestep $t$, is given to the agent. Notice, that the state and reward which are received in $t+1$ are a result of an action taken at timestep $t$, so the reward is not instantaneous but delayed.  \n",
    "\n",
    "\n",
    "If the random variable $S_t$ takes value $s$ we use the abbreviation $s_t$. \n",
    "\n",
    "The MDP and agent together give rise to a sequence called *__trajectory__* that begins like this:\n",
    "<p>\n",
    "$$s_{0}, a_{0}, r_{1},s_{1}, a_{1}, r_{2}, s_{2}, a_{2}, r_{3}, \\dots $$\n",
    "<p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function\n",
    "\n",
    "From the definition we also have a _reward function_ $\\mathcal R(s,a)$ which is the expected reward for taking action $a$ in state $s$:\n",
    "\n",
    "$$ \\mathcal R(s,a) = \\mathbb E [R \\mid S = s, A =a]=\\int_{\\mathbb R} \\sum_{\\mathcal s' \\in S} r \\cdot P_0(s',r \\mid s,a) dr $$\n",
    "\n",
    "We will explicit represent the function as matrix, so we will use the notation $\\mathcal R_s^a$.\n",
    "\n",
    "Analog we also have a function (of three variables) if the next state $S_{t+1}=s'$ at time $t+1$ is also given:\n",
    "\n",
    "$$ \\mathcal R(s, a, s') = \\mathcal R_{s,s'}^a =\\mathbb E [R_{t+1} \\mid S_t = s, A_t =a, S_{t+1}=s'] = \\int_{\\mathbb R} r \\cdot P_0(s', r \\mid s,a) dr $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property\n",
    "A state $S_t=s$ includes all information about the past to predict future states. Additional information about the past such as previous states and actions doesn't give more information about future states.\n",
    "\n",
    "In other word the states are Markovian, i.e.:\n",
    "\n",
    "$$ \\mathbb P[S_{t+1}, R_{t+1} \\mid S_t, A_t] = \\mathbb P[S_{t+1}, R_{t+1} \\mid S_1, A_1 \\dots , S_{t-1}, A_{t-1}, S_{t}, A_{t}] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State transition probability kernel\n",
    "From $\\mathcal P_0$ we can derive the state transition probability kernel $\\mathcal P$ which for any $(s, a, s') \\in \\mathcal S \\times \\mathcal A \\times \\mathcal S$ triple gives the probability of moving from state $s$ to some other state $s'$ provided that action $a$ was chosen in $s$:\n",
    "\n",
    "$$ P(s,a,s') = P_0(\\{s'\\} \\times \\mathbb R \\mid a, s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Jacks Car Rental \n",
    "(from [[SUT98]](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf))\n",
    "\n",
    "\"Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited $\\$10$ by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of $\\$2$ per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables with mean $\\lambda$. Suppose $\\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight.\"\n",
    "\n",
    "* if more cars should be moved from a location as available, then it will charged the full action but only the available cars are moved.\n",
    "* cars can be rent and returned at each location at all nationwide locations.\n",
    "\n",
    "\n",
    "* We call the first location `A` and the second location `B`\n",
    "\n",
    "__We want to answer the following question:__\n",
    "* What is the optimal transfer policy of cars between branches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_RATE = (3., 4.)      \n",
    "RETURN_RATE  = (3., 2.)\n",
    "\n",
    "GAMMA = 0.9\n",
    "RENTAL_INCOME = 10\n",
    "TRANSFER_COST = 2\n",
    "TRANSFER_MAX  = 5\n",
    "MAX_CAPACITY  = 20\n",
    "\n",
    "# location indicies\n",
    "A = 0\n",
    "B = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "__Question:__  \n",
    "* What are the valid states of the Jacks car rental enviroment?\n",
    "\n",
    "__Answer:__\n",
    "* Number of cars in each location ready for rent (e.g. the tuple (3,5) means 3 cars in location A and 5 cars in location B).\n",
    "\n",
    "### Probability to be in a state\n",
    "The probability for the states can be represented as a vector. For Jacks Car Rental:\n",
    "\n",
    "$$ p(\\vec s) = \\begin{pmatrix} p(S=s_0) \\\\ p(S=s_1) \\\\ p(S=s_2) \\\\ \\dots \\\\ p(S=s_{21}) \\\\ p(S=s_{22}) \\\\ \\dots \\\\ p(S=s_{440}) \\end{pmatrix} \\begin{matrix}(0,0) \\\\ (0,1) \\\\ (0,2)\\\\ \\dots \\\\ (1,0) \\\\ (1,1) \\\\ \\dots \\\\ (20,20) \\end{matrix} $$\n",
    "\n",
    "For the Jacks Car Rental problem it's sometimes easier to work with a state probability matrix:\n",
    "\n",
    "$$ P(\\hat S) = \\begin{pmatrix} P(S=s_{0}) & P(S=s_{1})& \\dots & P(S=s_{20}) \\\\ P(S=s_{21}) & P(S=s_{22}) & \\dots & P(S=s_{41})\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ . & . & \\dots & p(S=s_{440}) \\end{pmatrix} $$\n",
    "\n",
    "The indices of $P(\\hat S)$ correspond directly to the probability of the number of cars at each location.  \n",
    "Note that we can use `np.reshape` to switch between the different representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ = np.zeros([MAX_CAPACITY+1, MAX_CAPACITY+1])\n",
    "state_[11, 15] = 1. # 11 cars at A and 15 cars at B\n",
    "state_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all content of state_\n",
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "state = state_.reshape(-1)\n",
    "print(state.shape)\n",
    "state_11_15 = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to get the most probable location of a given state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_probable_location(state):\n",
    "    i = state.argmax()\n",
    "    return (i//(MAX_CAPACITY+1), i%(MAX_CAPACITY+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_probable_location(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to get the state vector for given $a$ and $b$, where $a$ is the amount of cars at $A$ and $b$ is the amount of cars at $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_vector(a, b):\n",
    "    s = np.zeros((MAX_CAPACITY+1)**2)\n",
    "    s[a*(MAX_CAPACITY+1)+b] = 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_most_probable_location(get_state_vector(11,12)) == (11,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Actions are the nightly movements of the cars. We encode them as a number between -5 to 5, e.g.\n",
    "\n",
    "* Action +3: move three cars from A to B.\n",
    "* Action -1: move one car from B to A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = np.arange(-TRANSFER_MAX, TRANSFER_MAX+1)\n",
    "print(action_space)  # cars moved  \n",
    "print(np.arange(11)) # action indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State transition probability kernel for Jacks Car Rental\n",
    "Definition\n",
    "$$ \\vec s' = P \\vec s $$\n",
    "\n",
    "with\n",
    "\n",
    "* $\\vec s$: probabilities for the states at time $t$\n",
    "* $\\vec s'$: probabilities for the states at time $t+1$\n",
    "* $P = \\mathcal P $: State Transition Probability Kernel\n",
    "\n",
    "In the following we construct the (transpose of the) _state transition probability kernel_. First note that the kernel decomposes as\n",
    "\n",
    "$$ P = P_{ret} P_{req} P_{move} $$\n",
    "\n",
    "with\n",
    "\n",
    "* $P_{move}$: The kernel for the nightly moves according to the actions.\n",
    "* $P_{req}$: The kernel for the requests of cars.\n",
    "* $P_{ret}$: The kernel for the returns of cars.\n",
    "\n",
    "__Note:__ for $P_{req}$ and $P_{ret}$ we don't have actions.\n",
    "\n",
    "\n",
    "Form of the Transition Matrix: \n",
    " - for earch old state we have a column\n",
    " - for each new states we have a row\n",
    "\n",
    "__Note:__  \n",
    "Here we have 441 possible states (for the old and new ones). So the transition matrix has dimension (441, 441).\n",
    "Each column of the transition matrix describes how a states is transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request State Transition Probability Kernel\n",
    "We define a function that calculates the transitions for one given location. Therefore, the Poisson probability mass function (PMF) is used. For further information about Poisson distribution in general, see e.g. [here](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459?gi=3b06308122e5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct request transitions for one location:\n",
    "# e.g. state_1 = ((0,*), (1,*), ... (20,*)) \n",
    "#     are only the probabilities for location 1\n",
    "\n",
    "MAX_PMF = 30\n",
    "\n",
    "def get_request_transitions_for_one_location(loc):\n",
    "    \"\"\"\n",
    "    Construct transition matrix P_{to, from} for one location only for requests.\n",
    "    The matrix has form (21, 21). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loc : int\n",
    "        Location: 0 or 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        request transition matrix\n",
    "\n",
    "    \"\"\"\n",
    "    assert(loc==A or loc==B)\n",
    "    # transition matrix P_{to, from} for one location only requests\n",
    "    transition_matrix = np.zeros([MAX_CAPACITY+1, MAX_CAPACITY+1])\n",
    "    \n",
    "    request_pmf = poisson.pmf(np.arange(MAX_PMF), REQUEST_RATE[loc])\n",
    "    np.testing.assert_almost_equal(request_pmf[-1], 0., decimal=12)\n",
    "    for i in range(MAX_CAPACITY+1):  \n",
    "        for j in range(MAX_CAPACITY+1):  \n",
    "            if j==0:\n",
    "                transition_matrix[i,j] = request_pmf[i:].sum()\n",
    "            elif j<=i:    \n",
    "                transition_matrix[i,j] = request_pmf[i-j]             \n",
    "    return transition_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_request_A_one_loc = get_request_transitions_for_one_location(A)\n",
    "# all colums should sum to one\n",
    "np.testing.assert_allclose(P_request_A_one_loc.sum(axis=0), 1.)\n",
    "P_request_A_one_loc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have e.g. eight cars at location A, after the renting the requested cars we get the following distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(MAX_CAPACITY+1), P_request_A_one_loc[:,8], '*b')\n",
    "plt.xticks(np.arange(0, 21, step=1))\n",
    "plt.xlabel(\"number of cars\")\n",
    "plt.ylabel(\"probability\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_request_A_one_loc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_request_A_one_loc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing the one location transition matricies\n",
    "s = np.zeros(MAX_CAPACITY+1)\n",
    "s[3] = 1\n",
    "a = np.dot(P_request_A_one_loc, s)\n",
    "np.testing.assert_almost_equal(a.sum(), 1.)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the defined matrix has shape 21x21. So the index considers only the location A. \n",
    "A full state is given by the pair \"(location A, location B)\". So we must extend the matrix to a \n",
    "\"full\" transition matrix 441x441.\n",
    "\n",
    "We define a function to retrieve the \"full\" transition matrix A for a given location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_transition_matrix_A(transition_one_loc):\n",
    "    block_size = MAX_CAPACITY+1 # for convenience\n",
    "    transition_matrix = np.zeros([block_size**2, block_size**2])\n",
    "    for i in range(block_size):\n",
    "        transition_matrix[i:block_size**2: block_size,\n",
    "                          i:block_size**2: block_size] = transition_one_loc\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_request_A = full_transition_matrix_A(P_request_A_one_loc)\n",
    "P_request_A#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should mix only states of A: \n",
    "np.testing.assert_almost_equal(np.dot(P_request_A, state_11_15).reshape(MAX_CAPACITY+1,-1).sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_request_A, state_11_15).reshape(MAX_CAPACITY+1,-1)[:,15].sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_request_A, state_11_15).reshape(MAX_CAPACITY+1,-1)[:12,15].sum(), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to retrieve the full transition matrix B for a given location. \n",
    "Note, that the positions of equal B locations in the full state vector a different a for location A.\n",
    "\n",
    "Full state vector:\n",
    "$$\\begin{bmatrix}(0,0) \\\\ (0,1) \\\\ (0,2)\\\\ \\dots \\\\ (1,0) \\\\ (1,1) \\\\ \\dots \\\\ (20,20) \\end{bmatrix} $$\n",
    "\n",
    "e.g. \n",
    "- indices for location A=0: $0,1, \\dots, 20$\n",
    "- indices for location B=0: $0,21,42, \\dots, 420$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_transition_matrix_B(transition_one_loc):\n",
    "    block_size = MAX_CAPACITY+1 # for convenience\n",
    "    transition_matrix = np.zeros([block_size**2, block_size**2])\n",
    "    for i in range(block_size):\n",
    "        transition_matrix[i*block_size:(i*block_size)+block_size,\n",
    "                          i*block_size:(i*block_size)+block_size] = transition_one_loc\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_request_B_one_loc = get_request_transitions_for_one_location(1)\n",
    "P_request_B = full_transition_matrix_B(P_request_B_one_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should mix only states of B: \n",
    "np.testing.assert_almost_equal(np.dot(P_request_B, state_11_15).reshape(MAX_CAPACITY+1,-1).sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_request_B, state_11_15).reshape(MAX_CAPACITY+1,-1)[11].sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_request_B, state_11_15).reshape(MAX_CAPACITY+1,-1)[11,:16].sum(), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the two request matricies for each location we can construct the full request matrix\n",
    "P_request = np.dot(P_request_A, P_request_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The order of application shouldn't matter, so the commutator should be zero:\n",
    "np.testing.assert_allclose(np.dot(P_request_A, P_request_B), np.dot(P_request_B, P_request_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return State Transition Probability Kernel\n",
    "\n",
    "Similar to the requests, we define the return state transition probability kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PMF = 30\n",
    "\n",
    "def get_return_transition_matrix_one_location(loc):\n",
    "    \"\"\"\n",
    "    Construct transition matrix P_{to, from} for one location only for returns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loc : int\n",
    "        Location: 0 or 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        transition matrix\n",
    "\n",
    "    \"\"\"\n",
    "    assert(loc==0 or loc==1)\n",
    "    transition_matrix = np.zeros([MAX_CAPACITY+1, MAX_CAPACITY+1])\n",
    "\n",
    "    return_pmf = poisson.pmf(np.arange(MAX_PMF), RETURN_RATE[loc])\n",
    "    np.testing.assert_almost_equal(return_pmf[-1], 0., decimal=12)\n",
    "    for i in range(MAX_CAPACITY+1):  \n",
    "        for j in range(MAX_CAPACITY+1):  \n",
    "            if j==MAX_CAPACITY:\n",
    "                transition_matrix[i,j] = return_pmf[j-i:].sum()\n",
    "            elif j>=i and j<MAX_CAPACITY:    \n",
    "                transition_matrix[i,j] = return_pmf[j-i]     \n",
    "    return transition_matrix.T\n",
    "\n",
    "P_return_A_one_loc = get_return_transition_matrix_one_location(0)\n",
    "np.testing.assert_almost_equal(P_return_A_one_loc.sum(axis=0), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_return_A = full_transition_matrix_A(P_return_A_one_loc)\n",
    "\n",
    "# should mix only states of A: \n",
    "np.testing.assert_almost_equal(np.dot(P_return_A, state_11_15).reshape(MAX_CAPACITY+1,-1).sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_return_A, state_11_15).reshape(MAX_CAPACITY+1,-1)[:,15].sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_return_A, state_11_15).reshape(MAX_CAPACITY+1,-1)[11:,15].sum(), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.zeros(MAX_CAPACITY+1)\n",
    "s[17] = 1\n",
    "a = np.dot(P_return_A_one_loc, s)\n",
    "st = np.dot(P_return_A , get_state_vector(17, 5))\n",
    "np.testing.assert_almost_equal(a,st.reshape(MAX_CAPACITY+1,-1)[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_return_B_one_loc = get_return_transition_matrix_one_location(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_return_B = full_transition_matrix_B(P_return_B_one_loc)\n",
    "\n",
    "# should mix only states of B: \n",
    "np.testing.assert_almost_equal(np.dot(P_return_B, state_11_15).reshape(MAX_CAPACITY+1,-1).sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_return_B, state_11_15).reshape(MAX_CAPACITY+1,-1)[11].sum(), 1.)\n",
    "np.testing.assert_almost_equal(np.dot(P_return_B, state_11_15).reshape(MAX_CAPACITY+1,-1)[11,15:].sum(), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_return = np.dot(P_return_B, P_return_A)\n",
    "\n",
    "np.testing.assert_allclose(np.dot(P_return_B, P_return_A), np.dot(P_return_A, P_return_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combinations of requests and returns is given by the dot product:\n",
    "\n",
    "$$ P_{ret/req} = P_{ret} P_{req} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_return_request = np.dot(P_return, P_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nightly Moves State Transition Probability Kernel\n",
    "We implement $P_{move}$ as a 3d numpy array with indices [to_state_index, from_state_index, action_index]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moves(a, b, action):\n",
    "    if action > 0: # from A to B\n",
    "        return min(a, action)\n",
    "    else:\n",
    "        return max(-b, action)\n",
    "\n",
    "def get_nightly_moves():\n",
    "    transition_matrix = np.zeros([(MAX_CAPACITY+1)**2, (MAX_CAPACITY+1)**2, action_space.shape[0]])\n",
    "    for a in range(MAX_CAPACITY+1):\n",
    "        for b in range(MAX_CAPACITY+1):\n",
    "            for i, action in enumerate(action_space):\n",
    "                old_state_index = a*(MAX_CAPACITY+1)+b\n",
    "                moves = get_moves(a, b, action)\n",
    "                new_a = min(a - moves, MAX_CAPACITY)\n",
    "                new_b = min(b + moves, MAX_CAPACITY)\n",
    "                new_state_index = new_a *(MAX_CAPACITY+1) + new_b\n",
    "                transition_matrix[new_state_index, old_state_index, i] = 1.\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "actions and indices for actions differ, e.g. action 2 has index 2+5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_move = get_nightly_moves()\n",
    "\n",
    "np.testing.assert_allclose(P_move.sum(axis=0), 1.)\n",
    "\n",
    "# check some moves\n",
    "# assert P_move[:,21*car_at_A+cars_at_B,action+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[new_cars_at_A, new_cars_at_B] == 1.\n",
    "assert P_move[:,0,0].reshape(MAX_CAPACITY+1, -1)[0,0] == 1.\n",
    "\n",
    "# e.g. from state [1,0] and action 1 => new state should be  [0,1]\n",
    "assert P_move[:,21*1+0,1+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[0,1] == 1. \n",
    "\n",
    "assert P_move[:,21*1+1,-2+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[2,0] == 1. \n",
    "assert P_move[:,21*9+5,0+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[9,5] == 1. \n",
    "assert P_move[:,21*9+5,3+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[6,8] == 1. \n",
    "assert P_move[:,21*9+5,-3+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[12,2] == 1.\n",
    "assert P_move[:,21*20+20,5+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[15,20] == 1. \n",
    "assert P_move[:,21*20+20,-4+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[20,16] == 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_move.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the full transition probability kernel (request, returns and nightly shifts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the transpose of the state transition probability kernel\n",
    "P = np.ndarray(((MAX_CAPACITY+1)**2, (MAX_CAPACITY+1)**2, action_space.shape[0]))\n",
    "for i in range(action_space.shape[0]): # TODO: without a loop?\n",
    "    P[:,:,i] = np.dot(P_return_request, P_move[:,:,i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P$ is the full state transition probability kernel for the Jack's car rental problem.\n",
    "\n",
    "$P$ has the following indices: $(s', s, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(P.sum(axis=0), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_a in range(21):\n",
    "    for s_b in range(21):\n",
    "        for a in range(11):\n",
    "            s_ = np.dot(P[:,:,a], get_state_vector(s_a, s_b))\n",
    "            np.testing.assert_almost_equal(s_.sum(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(P.sum(axis=0), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d_over_states(f, zlabel=\"\", ):\n",
    "    A = np.arange(0, MAX_CAPACITY+1)\n",
    "    B = np.arange(0, MAX_CAPACITY+1)\n",
    "    # B, A !!!\n",
    "    B, A = np.meshgrid(B, A)\n",
    "    V = f.reshape(MAX_CAPACITY+1,-1)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    #ax = fig.gca(projection='3d')\n",
    "    #surf = ax.plot_surface(A, B, V, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "    #                   linewidth=0, antialiased=False)\n",
    "    #fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax.scatter(A, B, V, c='b', marker='.')\n",
    "    ax.set_xlabel(\"cars at A\")\n",
    "    ax.set_ylabel(\"cars at B\")\n",
    "    ax.set_zlabel(zlabel)\n",
    "    \n",
    "    ax.set_xticks(np.arange(0,21,1))\n",
    "    ax.set_yticks(np.arange(0,21,1))\n",
    "    #plt.xticks(np.arange(0,1,21))\n",
    "    #ax.view_init(elev=10., azim=10)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do action 3 in cars_at_loc (10,5), i.e move three car from A to B:\n",
    "s_ = np.dot(P[:,:,3+TRANSFER_MAX], get_state_vector(10, 5))\n",
    "plot3d_over_states(s_, 'probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ = state_11_15\n",
    "for i in range (500):\n",
    "    s_ = np.dot(P[:,:, 1 + TRANSFER_MAX], s_)\n",
    "    \n",
    "print (get_most_probable_location(s_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function\n",
    "Let's construct the expected reward aka reward function\n",
    "\n",
    "$$ \\mathcal R(s,a) = \\mathcal R_s^a = \\mathbb E[R \\mid s, a] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward():\n",
    "    \n",
    "    poisson_mask = np.zeros((2, MAX_CAPACITY+1, MAX_CAPACITY+1))\n",
    "    po = (poisson.pmf(np.arange(MAX_CAPACITY+1), REQUEST_RATE[A]),\n",
    "          poisson.pmf(np.arange(MAX_CAPACITY+1), REQUEST_RATE[B]))\n",
    "    for loc in (A,B):\n",
    "        for i in range(MAX_CAPACITY+1):\n",
    "            poisson_mask[loc, i, :i] = po[loc][:i]\n",
    "            poisson_mask[loc, i, i] = po[loc][i:].sum()\n",
    "    # the poisson mask contains the probability distribution for renting x cars (x column) \n",
    "    # in each row j, with j the number of cars available at the location\n",
    "\n",
    "    reward = np.zeros([MAX_CAPACITY+1, MAX_CAPACITY+1, 2*TRANSFER_MAX+1])\n",
    "    for a in range(MAX_CAPACITY+1):\n",
    "        for b in range(MAX_CAPACITY+1):\n",
    "            for action in range(-TRANSFER_MAX, TRANSFER_MAX+1):\n",
    "                moved_cars = min(action, a) if action>=0 else max(action, -b)\n",
    "                a_ = a - moved_cars\n",
    "                a_ = min(MAX_CAPACITY, max(0, a_))\n",
    "                b_ = b + moved_cars\n",
    "                b_ = min(MAX_CAPACITY, max(0, b_))\n",
    "                reward_a = np.dot(poisson_mask[A, a_], np.arange(MAX_CAPACITY+1)) \n",
    "                reward_b = np.dot(poisson_mask[B, b_], np.arange(MAX_CAPACITY+1))     \n",
    "                reward[a, b, action+TRANSFER_MAX] = ( \n",
    "                            (reward_a + reward_b) * RENTAL_INCOME -\n",
    "                            np.abs(action) * TRANSFER_COST )\n",
    "                #if a==20 and b==20 and action==0:\n",
    "                #    print (a_,b_, action)\n",
    "                #    print (reward_a, reward_b)\n",
    "                #    print (reward[a, b, action+TRANSFER_MAX])\n",
    "        \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward = get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward = Reward.reshape(441,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "A policy $\\pi$ is the behaviour of an agent. It maps states $s$ to actions $a$:\n",
    "\n",
    "* Deterministic Policies: $a = \\pi(s)$\n",
    "* Stochastic Policies: $\\pi(a\\mid s) = P(a\\mid s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Reward Process\n",
    "A stationary policy $\\pi$ and a MDP induce a MRP. Choose always the action $a$ according to the policy $\\pi$.\n",
    "\n",
    "A Markov Reward Process (MRP) is a tuple $\\langle \\mathcal{S}, \\mathcal P_0'\\rangle$ with\n",
    "\n",
    "* $\\mathcal S$: finite set of states $s$\n",
    "* $\\mathcal P'_0$: state transition Kernel matrix: $\\mathcal P'_0$ assigns to each state $(S=s) \\in \\mathcal S$ a probability measure over $\\mathcal S \\times \\mathbb R$: $P'_0(\\cdot \\mid s)$ with the following semantics: For $U \\in \\mathcal S \\times \\mathbb R$ is $P'_0(U \\mid s)$ the probability that the next states and the reward $R$ belongs to the set $U$ given that the current state is $s$.\n",
    "\n",
    "This implies again a reward function $\\mathcal R^\\pi(s)$ resp. $\\mathcal R^\\pi_s$ as for the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Transition Probability Matrix for the MRP\n",
    "$$ \\mathcal P_{ss'}^\\pi = \\mathbb P [S_{t+1} = s' \\mid S_t = s , a = \\pi(s)] $$\n",
    "\n",
    "$$ \\mathcal P^\\pi = \\begin{pmatrix} \\mathcal P_{11} & \\dots & \\mathcal P_{1n} \\\\ \\vdots & & \\\\ \\mathcal P_{n1} & \\dots & \\mathcal P_{nn} \\end{pmatrix}$$\n",
    "\n",
    "Note that the indicies of the matrix are $ss'$ in contrast to the above constructed state transition kernel which has the indicies $s'sa$.\n",
    "\n",
    "Given a MDP $\\langle \\mathcal{S, A, \\mathcal P_o} \\rangle$ and a policy $\\pi$:\n",
    "* The state sequence $s_1, s_2, \\dots $ is a Markov process\n",
    "* The state and reward sequence $s_1, r_2, s_2, \\dots $ is a Markov Reward Process where\n",
    "* $\\mathcal P^\\pi_{s,s'} = \\sum_{a\\in \\mathcal A} \\pi(a \\mid s) \\mathcal P^a_{s,s'}$\n",
    "* $\\mathcal R^\\pi_{s} = \\sum_{a\\in \\mathcal A} \\pi(a \\mid s) \\mathcal R^a_{s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose in each state action 2 (move two cars from A to B)\n",
    "policy = np.ones((MAX_CAPACITY+1)**2, dtype=int) * 2\n",
    "policy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_kernel_for_policy(policy):\n",
    "    # use advanced indexing to get the entiers \n",
    "    return P[:, range((MAX_CAPACITY+1)**2), policy+TRANSFER_MAX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pi = get_transition_kernel_for_policy(policy)\n",
    "np.testing.assert_allclose(P_pi.sum(axis=0), 1.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_P_reward_for_policy(policy):\n",
    "    P_pi = get_transition_kernel_for_policy(policy)\n",
    "    return P_pi, Reward[range((MAX_CAPACITY+1)**2), policy+TRANSFER_MAX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_P_reward_for_policy(policy)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones((MAX_CAPACITY+1)**2, dtype=int) * 5\n",
    "P_pi, reward = get_P_reward_for_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_over_states(reward, 'avg reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Discounted Reward\n",
    "The _return_ $G_t$ is defined as the total discounted reward:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} $$\n",
    "\n",
    "#### Sum of discounted rewards is finite\n",
    "Note that with all rewards $R_t>0$\n",
    "\n",
    "$$ \\sum_{t=0}^\\infty R_t = \\infty $$\n",
    "\n",
    "Independent of the actual values of $R_t$. So we can't compare the (expected) reward of different states respectivly policies (which cases the state sequence).\n",
    "\n",
    "But $$ \\sum_{t=0}^\\infty \\gamma^t R_t $$\n",
    "\n",
    "with the discount $\\gamma$\n",
    "\n",
    "$0<\\gamma<1$\n",
    "\n",
    "has as result a finite number, because with the maximum reward $R_{max} = \\max_t (R_t)$ in the serie it is bounded by \n",
    "\n",
    "$$ \\sum_{t=0}^\\infty \\gamma^t R_t \\leq \\sum_{t=0}^\\infty \\gamma^t R_{max} = \\frac{R_{max}}{1-\\gamma} $$\n",
    "\n",
    "#### Proof of $\\sum_i \\gamma^i = 1/(1-\\gamma)$\n",
    "\n",
    "$$ x = \\gamma^0 +\\gamma^1 + \\gamma^2 + \\dots $$\n",
    "\n",
    "$$ \\gamma^0 +\\gamma^1 + \\gamma^2 + \\dots = \\gamma^0 + \\gamma (\\gamma^0 +\\gamma^1 + \\gamma^2 + \\dots) $$\n",
    "\n",
    "$$ x = \\gamma^0 + \\gamma x = 1 + \\gamma x $$\n",
    "\n",
    "$$ x(1-\\gamma) = 1 $$\n",
    "\n",
    "$$ x = \\frac{1}{1-\\gamma} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value functions\n",
    "The **state-value function** $v_\\pi (s)$ of an MDP is the expected return starting from state $s$, and then follow policy $\\pi$\n",
    "\n",
    "$$ v_\\pi (s) = \\mathbb E_\\pi[G_t \\mid S_t=s] = \\mathbb E \\left[ \\sum_{t'=t}^\\infty \\gamma^{t'-t} R_{t'+1} \\mid S_t=s \\right], s \\in \\mathcal S $$\n",
    "\n",
    "The value function can be decomposed into an immediate part and the discounted value function of the successor states:\n",
    "\n",
    "$$ v_\\pi(s) = \\mathbb E[G_t \\mid S_t =s] \\\\ = \\mathbb E [R_{t+1}+γR_{t+2}+\\gamma^2 R_{t+3} + \\dots \\mid S_t =s] \\\\ = \\mathbb E [R_{t+1} + \\gamma (R_{t+2}+\\gamma R_{t+3} + \\dots ) \\mid S_t = s] \\\\ = \\mathbb E [R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] \\\\ = \\mathbb E [ R_{t+1} \\mid S_t = s]+ \\gamma v_\\pi(S_{t+1} ) $$\n",
    "\n",
    "so, we have the __Bellmann Equation__ (for deterministic policies): \n",
    "\n",
    "$$ v_\\pi(s) = \\mathcal R_s^\\pi + \\gamma \\sum_{s'\\in \\mathcal S} P_{ss'}^\\pi v_\\pi (s') $$\n",
    "\n",
    "Note: $v(s')$ are the values of the successor states of $s$, so in $\\sum_{s'\\in \\mathcal S} P_{ss'} v(s')$ the weights $P_{ss'}$ of the values $v(s')$ are the transitions back in time.\n",
    "\n",
    "#### Bellman Equation\n",
    "also for non deterministic policies:\n",
    "\n",
    "$$ v_\\pi(s) = \\sum_a \\pi(a \\mid s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P^a_{s,s'} v_\\pi(s')\\right) $$\n",
    "\n",
    "Bellmann equations in vector-matrix form: \n",
    "\n",
    "$$ \\vec v = \\mathcal R^{\\pi} + \\gamma P^{\\pi} \\vec v $$\n",
    "\n",
    "Here, we can directly solve the Bellmann equation: \n",
    "\n",
    "$$ \\vec v = (I - \\gamma \\mathcal P^\\pi)^{-1} \\mathcal R^\\pi $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy):\n",
    "    P_pi_transpose, reward = get_P_reward_for_policy(policy)\n",
    "    \n",
    "    ### Here we must use P_pi_transpose.T - transformation back in time to the previous state!\n",
    "    values = np.dot(np.linalg.inv(np.eye((MAX_CAPACITY+1)**2) - GAMMA * P_pi_transpose.T), reward)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_ = np.zeros(((MAX_CAPACITY+1)**2), dtype=int)\n",
    "values = evaluate_policy(policy_)\n",
    "plot3d_over_states(values, 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large state spaces computing the inverse is costly in terms of space and time. However, the bellman\n",
    "equation can be solved by an iteration:\n",
    "\n",
    "### Policy Evaluation\n",
    "Computing the state-value function $v^\\pi(s)$ for an arbitrary policy by an **iterative** application of Bellman expectation backup is called policy evaluation:\n",
    "\n",
    "* $v_1 \\rightarrow v_2 \\rightarrow v_3 \\rightarrow \\dots \\rightarrow v_\\pi$\n",
    "* loop until convergence (using *synchronous* backups)\n",
    "  * at each iteration $k+1$\n",
    "  * for all states $s \\in \\mathcal S$\n",
    "  * update $v_{k+1}(s)$ from $v_k(s')$, where $s'$ is a successor state of $s$\n",
    "\n",
    "$$ \\vec v_{k+1} = \\vec {\\mathcal R}^\\pi + \\gamma {\\mathcal P}^\\pi \\vec v_{k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_by_iteration(policy, values = np.zeros((MAX_CAPACITY+1)**2)):\n",
    "    P_pi, reward = get_P_reward_for_policy(policy)\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        new_values = reward + GAMMA * np.dot (P_pi.T, values)\n",
    "        if np.allclose(new_values, values, rtol=1e-07, atol=1e-10):\n",
    "            converged = True\n",
    "        values = new_values\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_ = evaluate_policy_by_iteration(policy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(values, values_, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Value Function\n",
    "\n",
    "$$ v^* (s) = \\max_\\pi v_\\pi(s) $$\n",
    "\n",
    "The optimal value $v^*(s)$ of state $s \\in \\mathcal S$ is defined as the highest achievable expected return when the process is started from state $s$. The function $v^* : \\mathcal S \\rightarrow \\mathbb R$ is called the *optimal value function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "* Starting with an arbitrary policy\n",
    "* Iteration until the values converge (to the optimal policy). Alternating sequence of\n",
    "  * _Policy Evaluation_ : Compute the values of the policy\n",
    "  * _Greedy Policy Improvement_: Use the \"best\" action in each state, i.e. chose the action which result in the highest return for each state. \n",
    "\n",
    "\n",
    "[\"Policy iteration often converges in surprisingly few iterations.\"](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node43.html) [SUT98]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Policy Improvement\n",
    "\n",
    "For _greedy policy improvement_ we need the model (transition probabilities) if we are using the value function $v(s)$: For each action $a$ we compute the probabilities of the next states and calculate the average value of the discounted return.\n",
    "\n",
    " \n",
    "If we start from state $s$ with action $a$, and then follow policy $\\pi$ we get as expected return \n",
    "\n",
    "$$ q_\\pi (s,a) = \\mathbb E \\left[ R\\mid s,a \\right]+ \\mathbb E \\left[ \\gamma \\sum_{s'\\in \\mathcal S} P_{ss'}^\\pi v_\\pi (s') \\right]\n",
    "$$\n",
    "\n",
    "$q_\\pi (s,a)$ is called the _action-value function_ $q_\\pi (s,a)$.\n",
    "\n",
    "Here, we compute $q$ explicitly for each state $s$ and action $a$ (respectivly in matrix form). For each state we chose the action that results in the highest value.  \n",
    "\n",
    "To understand why _Greedy Policy Improvement_ works read the section of Sutton's book about the [policy improvement](http://incompleteideas.net/book/first/ebook/node42.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_improve(values, P=P):\n",
    "    P_ = P.transpose(1, 0, 2) # we used the model for improvement\n",
    "    all_pure_states = np.eye((MAX_CAPACITY+1)**2)\n",
    "    new_states = np.dot(all_pure_states, P_) \n",
    "    q = np.dot(new_states.transpose(2,1,0), values) \n",
    "    q = q.T + Reward\n",
    "    policy_indices = np.argmax(q, axis=1)\n",
    "    policy = policy_indices - TRANSFER_MAX\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(policy):\n",
    "    values = evaluate_policy_by_iteration(policy)\n",
    "    not_converged = True\n",
    "    while not_converged:\n",
    "        print (values.mean())\n",
    "        new_policy = greedy_improve(values)\n",
    "        #new_values = evaluate_policy(new_policy)\n",
    "        new_values = evaluate_policy_by_iteration(new_policy, values)\n",
    "        if np.allclose(new_values, values, rtol=1e-02):\n",
    "            not_converged = False\n",
    "        values = new_values \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(policy):\n",
    "    A = np.arange(0, MAX_CAPACITY+1)\n",
    "    B = np.arange(0, MAX_CAPACITY+1)\n",
    "    A, B = np.meshgrid(A, B)\n",
    "    Po = policy.reshape(MAX_CAPACITY+1,-1)\n",
    "    levels = range(-5,6,1)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    CS = plt.contourf(A, B, Po, levels)\n",
    "    cbar = plt.colorbar(CS)\n",
    "    cbar.ax.set_ylabel('actions')\n",
    "    #plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.title('Policy')\n",
    "    plt.xlabel(\"cars at B\")\n",
    "    plt.ylabel(\"cars at A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = improve_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_ = evaluate_policy_by_iteration(policy)\n",
    "plot3d_over_states(values_, 'v') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Function\n",
    "\n",
    "The **action-value function** $q_\\pi (s,a)$ of an MDP is the expected return starting from state $s$ with action $a$, and then follow policy $\\pi$\n",
    "\n",
    "$$ q^\\pi (s,a) = \\mathbb E_\\pi[G_t \\mid S_t=s, A_t=a] $$\n",
    "\n",
    "### Optimal action-value function\n",
    "\n",
    "$$ q^* (s, a) = \\max_\\pi q^\\pi(s, a) $$\n",
    "\n",
    "The optimal action-value $q^*(s, a)$ of state-action pair $(s,a) \\in \\mathcal S \\times \\mathcal A$ gives the highest achievable expected return when the process is started from state $s$, and the first action chosen is $a$. The function $q^* : \\mathcal S \\times \\mathcal A \\rightarrow \\mathbb R$ is called the optimal action-value function.\n",
    "\n",
    "### Connection between optimal value- and action-value function\n",
    "\n",
    "$$ v^*(s) = \\max_{a \\in \\mathcal A} q^*(s,a), s \\in \\mathcal S $$\n",
    "\n",
    "$$ q^*(s,a) = \\mathcal R_s^a + \\gamma\\sum_{s' \\in \\mathcal S} P(s,a,s')v^*(s') ; s \\in \\mathcal S, a \\in \\mathcal A $$\n",
    "\n",
    "### Bellman Equation for Q\n",
    "\n",
    "$$ q_\\pi(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P^a_{s,s'} \\left( \\sum_{a'\\in \\mathcal A} \\pi(a' \\mid s') q_\\pi(s',a') \\right) $$\n",
    "\n",
    "for a deterministic policy: \n",
    "\n",
    "$$ q_\\pi(s, a) = \\mathcal R_s^a + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P^a_{s,s'} q_\\pi(s',\\pi(s')) $$\n",
    "\n",
    "$$ \\hat Q^\\pi = \\hat R + \\gamma \\mathcal P \\vec Q^\\pi_\\pi $$\n",
    "\n",
    "with\n",
    "\n",
    "* $\\vec \\pi$: Policy vector\n",
    "* Matricies $\\hat Q^\\pi, \\hat R$ with entries $\\cdot_{sa}$\n",
    "* 3d Array $\\mathcal P$ with entries $\\cdot_{sas'}$\n",
    "* $\\vec Q^\\pi_\\pi = \\vec v^\\pi$\n",
    "\n",
    "### Policy Evaluation for Q\n",
    "\n",
    "Computing the state-action-value function $q^\\pi(s, a)$ for an arbitrary policy by an **iterative** application of Bellman expectation backup is also called policy evaluation:\n",
    "\n",
    "* $q_1 \\rightarrow q_2 \\rightarrow q_3 \\rightarrow \\dots \\rightarrow q_\\pi$\n",
    "* using synchronous backups\n",
    "  * at each iteration $k+1$\n",
    "  * for all states $s \\in \\mathcal S$ and $a \\in \\mathcal A$\n",
    "  * update $q_{k+1}(s, a)$ from $q_k(s', \\cdot)$, where $s'$ is a successor state of $s$.\n",
    "\n",
    "$$ \\hat Q^{k+1} = \\hat R + \\gamma \\mathcal P \\vec Q^k_{k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative update of the bellmann equation for Q\n",
    "def evaluate_policy_Q_by_iteration(policy, Q = np.zeros([(MAX_CAPACITY+1)**2, 2*TRANSFER_MAX+1])):\n",
    "    converged = False\n",
    "    P_ = P.transpose(1,2,0)\n",
    "    while not converged:\n",
    "        policy_index = policy + TRANSFER_MAX\n",
    "        Q_pi = Q[np.arange((MAX_CAPACITY+1)**2), policy_index]\n",
    "        new_Q = Reward + GAMMA * np.dot(P_, Q_pi)\n",
    "        if np.allclose(new_Q, Q):\n",
    "            converged = True\n",
    "        Q = new_Q\n",
    "    return Q    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration for Q\n",
    "* Starting with an arbitrary policy\n",
    "* Iteration until the values converge (to the optimal policy). Alternating sequence of\n",
    "  * Policy Evaluation : Compute the values of the policy\n",
    "  * Greedy Policy Improvement:\n",
    "    * $\\pi'(s) = \\text{arg} \\max_{a \\in \\mathcal A} q^\\pi(s,a)$\n",
    "    * Note: Here we don't use the (transition) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_improve_Q(Q):\n",
    "    return np.argmax(Q, axis=1) - TRANSFER_MAX\n",
    "\n",
    "\n",
    "policy = np.ones((MAX_CAPACITY+1)**2, dtype=int) * 0\n",
    "Q = np.zeros([(MAX_CAPACITY+1)**2, 2*TRANSFER_MAX+1])\n",
    "\n",
    "converged=False\n",
    "while not converged:\n",
    "    new_Q = evaluate_policy_Q_by_iteration(policy, Q)\n",
    "    policy = greedy_improve_Q(Q)\n",
    "    if np.allclose(new_Q, Q):\n",
    "            converged = True\n",
    "    Q = new_Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimality Equation\n",
    "$$ v^*(s) = \\max_a \\mathbb E \\left[ R_{t+1} + \\gamma v^*(S_{t+1}) \\mid S_t = s, A_t = a\\right] \\\\ = \\max_a \\left(\\mathcal R^a_{s} + \\gamma \\sum_{s'} P_{ss'}^a \\mathcal v^*(S_{t+1}=s')\\right) $$\n",
    "\n",
    "$$ q^*(s, a) = \\mathbb E \\left[ R_{t+1} + \\gamma \\max_{a'} q^*(S_{t+1},A_{t+1}=a') \\mid S_t = s, A_t = a\\right] \\\\ = \\mathcal R^a_{s} + \\gamma \\sum_{s'} P_{ss'}^a \\mathcal \\max_{a'} q^*(S_{t+1}=s',A_{t+1}=a') $$\n",
    "\n",
    "### Value Iteration\n",
    "From the Bellmann optimality equation we can derive an iterative method to find the optimal value function:\n",
    "\n",
    "$$ v_{k+1}(S_t = s) = \\max_a \\mathbb E[R_{t+1} + \\gamma v_k(S_{t+1}=s') \\mid S_t=s, A_t=a] \\\\ = \\max_a \\left( \\mathcal R_{s}^a + \\gamma \\sum_{s'} \\mathcal P_{ss'}^a v_k(s')\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged = False\n",
    "values = np.zeros((MAX_CAPACITY+1)**2)\n",
    "P_ = P.transpose(1,2,0)\n",
    "while not converged:\n",
    "    rs = Reward + GAMMA * np.dot(P_, values) \n",
    "    new_values = np.max(rs, axis = 1)\n",
    "    if np.allclose(values, new_values):\n",
    "        converged = True\n",
    "    values = new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.argmax(Reward + GAMMA * np.dot(P_, values) , axis=1) - TRANSFER_MAX\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Outlook\n",
    "\n",
    "Here we introduced MDPs as a model of the agent-environment interaction. We showed how to construct an explicit\n",
    "transition model on an example of R. Suttons Book \"Introduction to Reinforcement Learning\".\n",
    "\n",
    "We also showed how to solve the corresponding optimization problem by dynamic programming.\n",
    "\n",
    "In the next section no explicit transition kernel will be given. However, the interaction of the environment and the\n",
    "agent is also defined by a MDP. However, without the explicit transition kernel only tranjectories (as data) from the interaction are known. Such problems can not be solved by dynamic programming. Here, techniques from reinforcement learning\n",
    "are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"SUT98\"></a>[SUT98]\n",
    "        </td>\n",
    "        <td>\n",
    "            Richard S. Sutton and Andrew G. Barto, \"Introduction to Reinforcement Learning\", MIT Press, Cambridge, MA, USA, 1998.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "_Introduction to Reinforcement Learning_  \n",
    "by  _Christian Herta_, _Patrick Baumann_\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018  _Christian Herta_, _Patrick Baumann_\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
